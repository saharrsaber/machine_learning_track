{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Natural Language Processing in Python\n",
    "\n",
    "It's the ... course in [DataCamp Machine Learning in Python Track](https://learn.datacamp.com/career-tracks/machine-learning-scientist-with-python?version=1). Katharine Jarmul is the instructor. She is the Founder of kjamistan\n",
    "\n",
    "the first part of this notebook contains the main Algorithms I learned from [the course](https://learn.datacamp.com/courses/introduction-to-natural-language-processing-in-python).  The second is how I implemented these algorithms into real-world Dataset\n",
    " \n",
    "## Part 1: Main Algorithms\n",
    "### Ch1. Regular expressions & word tokenization\n",
    "- Basics: \n",
    "<br> &emsp; Topic identification (Chatbox)\n",
    "<br> &emsp; Text Classification (Sentiment Analysis)\n",
    "\n",
    "- Regular Expression: import re\n",
    "<br> &emsp; \\d : digit\n",
    "<br> &emsp; \\w : letter\n",
    "<br> &emsp; \\s : white-space\n",
    "<br> &emsp; \\D : not-digit\n",
    "<br> &emsp; \\W : not letter\n",
    "\n",
    "- re.match(pattern, my_string)  VS  re.search(pattern, my_string)\n",
    "<br> &emsp; match -> only the begining of my_string\n",
    "<br> &emsp; search -> searhes all my_string\n",
    "\n",
    "- tokenization: from nltk.tokenize import ..\n",
    "<br> &emsp; word_tokenize\n",
    "<br> &emsp; sent_tokenize\n",
    "<br> &emsp; regexp_tokenize\n",
    "<br> &emsp; TweetTokenizer\n",
    "\n",
    "### Ch2. Simple topic identification\n",
    "- bag-of-word using Counter\n",
    "<br> &emsp; Counter(tokens).most_common()\n",
    "\n",
    "- preprocessing: ntlk.corpus.stopwords\n",
    "<br> &emsp; stopwords.words('english')\n",
    "\n",
    "- preprocessing: lemmatization\n",
    "<br> &emsp; WordNetLemmatizer.lemmatize(words)\n",
    "\n",
    "-  bag-of-word using gensim \n",
    "<br> &emsp; creates word vector (spare array display the relation between the words)\n",
    "<br> &emsp; Dictionary: map tokens to ids -> dic.token2id\n",
    "<br> &emsp; dic.doc2bow(article) [(id, freq), ... ]\n",
    "\n",
    "\n",
    "### Ch3. Named-entity recognition\n",
    "-  Named Entity Recognition\n",
    "<br> &emsp; regonize important named entity in text\n",
    "<br> &emsp; used to answer WHO? WHEN? WHERE? WHAT?\n",
    "\n",
    "- using SpaCy\n",
    "<br> &emsp; import spacy; nlp = spacy.laod('en'); doc = nlp(doc); doc.ents[0].text, doc.ents[0].label_\n",
    "\n",
    "- using polyglot \n",
    "<br> &emsp; from polyglot.text import Text; doc_entity = Text(doc);  doc_entity.entities; doc_entity.entities[0], tagdoc_entity.entities[0], \n",
    "\n",
    "\n",
    "### Ch4. Building a \"fake news\" classifier\n",
    "-  casestudy\n",
    "<br> &emsp;\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
